{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Assignment Code: DA-AG-016\n",
        "KNN & PCA | Assignment\n",
        "Instructions: Carefully read each question. Use Google Docs, Microsoft Word, or a similar tool to create a document where you type out each question along with its answer. Save the document as a PDF, and then upload it to the LMS. Please do not zip or archive the files before uploading them. Each question carries 20 marks.\n",
        "                                                               Total Marks: 200\n"
      ],
      "metadata": {
        "id": "BKyWEPuGg122"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 1: What is K-Nearest Neighbors (KNN) and how does it work in both\n",
        "classification and regression problems?\n",
        "\n",
        "Answer:K-Nearest Neighbors (KNN) is a simple, supervised learning algorithm that classifies or regresses new data points by finding the 'K' most similar points (neighbors) from the training data, using distance metrics like Euclidean distance; for classification, it uses majority vote of neighbors' labels, while for regression, it averages their continuous values. It's a \"lazy learner\" as it stores all data and learns during prediction, making it versatile but computationally heavy for large datasets.\n",
        "#KNN Works-\n",
        "1.Choose 'K': Select the number of neighbors (K) to consider.\n",
        "2.Calculate Distance: Measure the distance (e.g., Euclidean) between the new data point and all training points.\n",
        "3.Identify Neighbors: Find the 'K' training points closest to the new point.\n",
        "Predict:\n",
        "4.Classification: Assign the new point the most common class label among the K neighbors (majority vote).\n",
        "5.Regression: Calculate the average (mean) of the continuous values of the K neighbors to predict the new point's value."
      ],
      "metadata": {
        "id": "OWIYJ3J1hB2f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 2: What is the Curse of Dimensionality and how does it affect KNN\n",
        "performance?\n",
        "\n",
        "Answer:The Curse of Dimensionality describes how data becomes sparse and distances lose meaning in high-dimensional spaces, making algorithms struggle; for KNN, this means neighbors become far away, data needs explode, and models overfit due to noise, severely degrading performance as features increase. Essentially, the vast feature space requires exponentially more data to stay representative, and relevant neighbors become indistinguishable from noise, causing poor generalization\n",
        "\n",
        "How it Affects KNN Performance:\n",
        "Data Needs Skyrocket: To adequately cover the vast space and find reliable neighbors, KNN needs an unrealistic, often infinite, amount of training data.\n",
        "Neighbors Get Far Away: Due to sparsity, the \"nearest\" neighbors of a test point might be very distant, making the local neighborhood less representative and reliable.\n",
        "Overfitting: With too many features and sparse data, the algorithm can latch onto noise rather than true patterns, leading to excellent training performance but poor results on new data (poor generalization).\n",
        "Computational Cost: Calculating distances becomes computationally expensive and slow in high dimensions."
      ],
      "metadata": {
        "id": "Bc5C4EFkhB54"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 3: What is Principal Component Analysis (PCA)? How is it different from\n",
        "feature selection?\n",
        "\n",
        "Answer:Principal Component Analysis (PCA) is a feature extraction method that transforms original features into fewer, new uncorrelated features (Principal Components) capturing maximum data variance, while Feature Selection chooses a subset of original features, discarding the rest, often based on predictive power for a target, making PCA a transformation technique (new features) and Feature Selection a filtering technique (original features). PCA creates new features, losing interpretability; Feature Selection keeps original features, retaining interpretability but potentially losing some information if redundant features are kept."
      ],
      "metadata": {
        "id": "SmumbV0YhB-j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 4: What are eigenvalues and eigenvectors in PCA, and why are they\n",
        "important?\n",
        "\n",
        "Answer:In PCA, eigenvectors define the new axes (Principal Components) of maximum data variance, while eigenvalues quantify how much variance each component captures, indicating its importance. They are crucial because they allow us to reduce complex, high-dimensional data to fewer, more meaningful components by identifying directions of highest information, simplifying the dataset while preserving most variability, essential for dimensionality reduction.\n",
        "They're Important in PCA-\n",
        "1.Dimensionality Reduction: By ranking eigenvalues from largest to smallest, you identify the most significant components (eigenvectors) that explain most of the data's variability (e.g., 99%).\n",
        "2.Information Prioritization: You can discard components with small eigenvalues (less variance/information) and keep those with large ones, effectively reducing dimensions without losing much data quality.\n",
        "3.Feature Extraction: The new components (eigenvectors) are linear combinations of original variables, creating a new, compressed representation of the data that highlights key patterns."
      ],
      "metadata": {
        "id": "nAVyon2GhCBq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 5: How do KNN and PCA complement each other when applied in a single\n",
        "pipeline?\n",
        "Dataset:\n",
        "Use the Wine Dataset from sklearn.datasets.load_wine().\n",
        "\n",
        "Answer:In a machine learning pipeline using the Scikit-learn Wine dataset, Principal Component Analysis (PCA) and k-Nearest Neighbors (KNN) complement each other by addressing the specific limitations of distance-based algorithms in high-dimensional spaces.\n",
        "They complement each other-\n",
        "1.Mitigating the \"Curse of Dimensionality\": KNN relies on calculating distances (e.g., Euclidean) between points. In the Wine dataset’s 13-dimensional space, data points become sparse, making distances less meaningful. PCA reduces this dimensionality while retaining the most significant variance, which helps KNN find \"truer\" neighbors.\n",
        "2.Handling Multicollinearity: The Wine dataset contains several highly correlated chemical features, such as \"Flavanoids\" and \"Total Phenols\". PCA transforms these into uncorrelated principal components, preventing redundant information from disproportionately influencing KNN's distance calculations.\n",
        "3.Noise Reduction: PCA filters out low-variance components that often represent noise. By feeding only the most informative components into KNN, the model can achieve better generalization and sometimes higher accuracy than when using the raw features.\n",
        "4.Computational Efficiency: KNN is computationally expensive during prediction because it must calculate distances to all training points. Reducing the number of features through PCA significantly speeds up these calculations without a substantial loss in information."
      ],
      "metadata": {
        "id": "yEc29tn2hCEo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 6: Train a KNN Classifier on the Wine dataset with and without feature\n",
        "scaling. Compare model accuracy in both cases.\n",
        "(Include your Python code and output in the code box below.)\n",
        "Answer:"
      ],
      "metadata": {
        "id": "adfHtlzDhCHd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "from sklearn.datasets import load_wine\n",
        "wine_df = load_wine(as_frame=True).frame\n",
        "print(wine_df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fu7S1lF5h_yy",
        "outputId": "9f20851a-6669-4207-f8c5-f02d6ee50ab4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   alcohol  malic_acid   ash  alcalinity_of_ash  magnesium  total_phenols  \\\n",
            "0    14.23        1.71  2.43               15.6      127.0           2.80   \n",
            "1    13.20        1.78  2.14               11.2      100.0           2.65   \n",
            "2    13.16        2.36  2.67               18.6      101.0           2.80   \n",
            "3    14.37        1.95  2.50               16.8      113.0           3.85   \n",
            "4    13.24        2.59  2.87               21.0      118.0           2.80   \n",
            "\n",
            "   flavanoids  nonflavanoid_phenols  proanthocyanins  color_intensity   hue  \\\n",
            "0        3.06                  0.28             2.29             5.64  1.04   \n",
            "1        2.76                  0.26             1.28             4.38  1.05   \n",
            "2        3.24                  0.30             2.81             5.68  1.03   \n",
            "3        3.49                  0.24             2.18             7.80  0.86   \n",
            "4        2.69                  0.39             1.82             4.32  1.04   \n",
            "\n",
            "   od280/od315_of_diluted_wines  proline  target  \n",
            "0                          3.92   1065.0       0  \n",
            "1                          3.40   1050.0       0  \n",
            "2                          3.17   1185.0       0  \n",
            "3                          3.45   1480.0       0  \n",
            "4                          2.93    735.0       0  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 7: Train a PCA model on the Wine dataset and print the explained variance\n",
        "ratio of each principal component.\n",
        "(Include your Python code and output in the code box below.)\n",
        "Answer:"
      ],
      "metadata": {
        "id": "nqBq8GvShCKn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# 1. Load the Wine dataset\n",
        "wine = load_wine()\n",
        "X = wine.data\n",
        "\n",
        "# 2. Standardize the data (PCA is sensitive to feature scaling)\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# 3. Initialize and fit PCA for all 13 components\n",
        "pca = PCA()\n",
        "pca.fit(X_scaled)\n",
        "\n",
        "# 4. Print Explained Variance Ratio for each principal component\n",
        "print(\"Explained Variance Ratio per Principal Component:\")\n",
        "for i, ratio in enumerate(pca.explained_variance_ratio_):\n",
        "    print(f\"PC{i+1:2}: {ratio:.4f}\")\n",
        "\n",
        "# 5. Print Cumulative Explained Variance\n",
        "cumulative_variance = np.cumsum(pca.explained_variance_ratio_)\n",
        "print(f\"\\nTotal Cumulative Variance (13 components): {cumulative_variance[-1]:.2f}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J7V8jM6UiSvT",
        "outputId": "6b84955f-9926-48bc-b5b1-8c03043e7d8d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Explained Variance Ratio per Principal Component:\n",
            "PC 1: 0.3620\n",
            "PC 2: 0.1921\n",
            "PC 3: 0.1112\n",
            "PC 4: 0.0707\n",
            "PC 5: 0.0656\n",
            "PC 6: 0.0494\n",
            "PC 7: 0.0424\n",
            "PC 8: 0.0268\n",
            "PC 9: 0.0222\n",
            "PC10: 0.0193\n",
            "PC11: 0.0174\n",
            "PC12: 0.0130\n",
            "PC13: 0.0080\n",
            "\n",
            "Total Cumulative Variance (13 components): 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 8: Train a KNN Classifier on the PCA-transformed dataset (retain top 2\n",
        "components). Compare the accuracy with the original dataset.\n",
        "(Include your Python code and output in the code box below.)\n",
        "Answer:"
      ],
      "metadata": {
        "id": "C-fPIXJLhCN1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 1. Load dataset\n",
        "data = load_iris()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# 2. Standardize features (highly recommended for both PCA and KNN)\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# 3. Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# 4. Train and evaluate KNN on original dataset\n",
        "knn_original = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_original.fit(X_train, y_train)\n",
        "y_pred_orig = knn_original.predict(X_test)\n",
        "acc_original = accuracy_score(y_test, y_pred_orig)\n",
        "\n",
        "# 5. Apply PCA to retain top 2 components\n",
        "pca = PCA(n_components=2)\n",
        "X_train_pca = pca.fit_transform(X_train)\n",
        "X_test_pca = pca.transform(X_test) # Transform test data using training fit\n",
        "\n",
        "# 6. Train and evaluate KNN on PCA-transformed dataset\n",
        "knn_pca = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_pca.fit(X_train_pca, y_train)\n",
        "y_pred_pca = knn_pca.predict(X_test_pca)\n",
        "acc_pca = accuracy_score(y_test, y_pred_pca)\n",
        "\n",
        "# Output results\n",
        "print(f\"Accuracy with original dataset: {acc_original:.4f}\")\n",
        "print(f\"Accuracy with PCA (top 2 components): {acc_pca:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gw2CiF_Kk7hK",
        "outputId": "3915ad3e-cb84-4007-df7e-b9cbc73729e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy with original dataset: 1.0000\n",
            "Accuracy with PCA (top 2 components): 0.9556\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 9: Train a KNN Classifier with different distance metrics (euclidean,\n",
        "manhattan) on the scaled Wine dataset and compare the results.\n",
        "(Include your Python code and output in the code box below.)\n",
        "Answer:"
      ],
      "metadata": {
        "id": "oQ1nbxjThCQ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 1. Load the Wine dataset\n",
        "wine = load_wine()\n",
        "X, y = wine.data, wine.target\n",
        "\n",
        "# 2. Split into training and testing sets (80/20 split)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 3. Scale the features (Critical for KNN)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# 4. Train KNN with Euclidean distance\n",
        "knn_euclidean = KNeighborsClassifier(n_neighbors=5, metric='euclidean')\n",
        "knn_euclidean.fit(X_train_scaled, y_train)\n",
        "y_pred_euc = knn_euclidean.predict(X_test_scaled)\n",
        "acc_euc = accuracy_score(y_test, y_pred_euc)\n",
        "\n",
        "# 5. Train KNN with Manhattan distance\n",
        "knn_manhattan = KNeighborsClassifier(n_neighbors=5, metric='manhattan')\n",
        "knn_manhattan.fit(X_train_scaled, y_train)\n",
        "y_pred_man = knn_manhattan.predict(X_test_scaled)\n",
        "acc_man = accuracy_score(y_test, y_pred_man)\n",
        "\n",
        "print(f\"Accuracy (Euclidean): {acc_euc:.4f}\")\n",
        "print(f\"Accuracy (Manhattan): {acc_man:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q7MW_4yNnguE",
        "outputId": "a90e4589-f9e7-4cdb-d2b2-29947f4cc702"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy (Euclidean): 0.9444\n",
            "Accuracy (Manhattan): 0.9444\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 10: You are working with a high-dimensional gene expression dataset to\n",
        "classify patients with different types of cancer.\n",
        "Due to the large number of features and a small number of samples, traditional models\n",
        "overfit.\n",
        "Explain how you would:\n",
        "● Use PCA to reduce dimensionality\n",
        "● Decide how many components to keep\n",
        "● Use KNN for classification post-dimensionality reduction\n",
        "● Evaluate the model\n",
        "● Justify this pipeline to your stakeholders as a robust solution for real-world\n",
        "biomedical data\n",
        "(Include your Python code and output in the code box below.)\n",
        "Answer:"
      ],
      "metadata": {
        "id": "_77Nr_pkhCVY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# 1. Data Preparation (Mock high-dim gene data: 100 samples, 5000 genes)\n",
        "X = np.random.rand(100, 5000)\n",
        "y = np.random.choice([0, 1], size=100) # Binary cancer classes\n",
        "\n",
        "# 2. Preprocessing: Scaling is mandatory for PCA and KNN\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# 3. PCA: Retain 95% of variance\n",
        "pca = PCA(n_components=0.95)\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "# 4. Train-Test Split on the reduced space\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_pca, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 5. KNN Classification (k=5 is a common starting point)\n",
        "knn = KNeighborsClassifier(n_neighbors=5)\n",
        "knn.fit(X_train, y_train)\n",
        "\n",
        "# 6. Evaluation\n",
        "y_pred = knn.predict(X_test)\n",
        "print(f\"Components kept: {pca.n_components_}\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Output Example:\n",
        "# Components kept: 74\n",
        "#               precision    recall  f1-score   support\n",
        "#            0       0.56      0.50      0.53        10\n",
        "#            1       0.55      0.60      0.57        10\n",
        "#     accuracy                           0.55        20\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PXxTG05anbJf",
        "outputId": "5e14ba0f-8dda-4830-97d3-d98d9603257b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Components kept: 93\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.38      0.62      0.48         8\n",
            "           1       0.57      0.33      0.42        12\n",
            "\n",
            "    accuracy                           0.45        20\n",
            "   macro avg       0.48      0.48      0.45        20\n",
            "weighted avg       0.50      0.45      0.44        20\n",
            "\n"
          ]
        }
      ]
    }
  ]
}